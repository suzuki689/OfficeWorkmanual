{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suzuki689/OfficeWorkmanual/blob/main/SD4_HB_Section05_02_%E5%87%BA%E5%B8%AD%E7%95%AA%E5%8F%B7_%E9%88%B4%E8%80%92%E6%82%A0%E5%A4%AA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 高度プログラミング演習Ｂ プログラムテンプレート\n",
        "## 統一指示\n",
        "\n",
        "\n",
        "1.   以下の形式でファイル名の「出席番号_フルネーム」の箇所を自分にあわせて変更してください\n",
        "2.   メニューの[ファイル]->[ドライブにコピーを保存]から自分のGoogleDrive内にコピーを保存してから、編集してください。最後に指示があったら、メニューの[ファイル]->[Githubにコピーを保存]からGithubOrganizationの25SD4組織内にコピーを保存・提出してください。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lsy7tjsJ5Dt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain langchain-google-genai python-dotenv"
      ],
      "metadata": {
        "id": "qaiYxNn6IMWj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# シークレットからAPIキーを読み込む\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "# 環境変数にも設定\n",
        "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n",
        "print(\"APIキーの読み込み完了。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRu4v7nSIdLD",
        "outputId": "99d2612e-7050-4fdc-a3bf-223a515bc209"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "APIキーの読み込み完了。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda"
      ],
      "metadata": {
        "id": "UP24B1mTInxx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. AIモデルの準備 (gemini-flash-latest を使用)\n",
        "model = ChatGoogleGenerativeAI(\n",
        " model=\"gemini-flash-latest\",\n",
        " temperature=0\n",
        ")\n",
        "# 2. プロンプトテンプレートの準備\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        " (\"system\", \"あなたは親切なアシスタントです。\"),\n",
        " (\"human\", \"{question}\")\n",
        "])\n",
        "# 3. 出力パーサーの準備\n",
        "output_parser = StrOutputParser()\n",
        "# 4. 自作のPython関数を定義\n",
        "def add_prefix_suffix(text: str) -> str:\n",
        " return f\"--- 回答開始 ---\\n{text}\\n--- 回答終了 ---\""
      ],
      "metadata": {
        "id": "cIAKcTPUIwAD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RunnableLambda を使って自作関数をチェーンに組み込む\n",
        "lambda_chain = (\n",
        " prompt\n",
        " | model\n",
        " | output_parser\n",
        " | RunnableLambda(add_prefix_suffix)\n",
        ")\n",
        "# チェーンを実行\n",
        "question = \"LangChainのLCELとは何ですか？\"\n",
        "response = lambda_chain.invoke({\"question\": question})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3bkun7pI1mw",
        "outputId": "f2d88558-6fb0-45b8-b182-6c4c47db3e91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 回答開始 ---\n",
            "LangChainの**LCEL（LangChain Expression Language）**は、LangChainフレームワークの中核をなす非常に重要な概念です。\n",
            "\n",
            "これは、複雑なチェーンやランナブル（実行可能なコンポーネント）を、直感的かつ効率的に構築、合成、実行するための仕組みです。\n",
            "\n",
            "親切なアシスタントとして、LCELがなぜ重要なのか、そしてどのような特徴があるのかを、分かりやすくご説明しますね。\n",
            "\n",
            "---\n",
            "\n",
            "## LCEL（LangChain Expression Language）とは？\n",
            "\n",
            "LCELは、LangChainで提供される、**コンポーネントをパイプラインとして結合するための標準的な方法**です。Pythonの`|`演算子（パイプ演算子）を使って、複数のステップを連結し、一つの実行可能なユニット（ランナブル）を作成します。\n",
            "\n",
            "### 1. なぜLCELが必要なのか？\n",
            "\n",
            "従来のプログラミングでは、LLMアプリケーションの各ステップ（プロンプトのフォーマット、LLMの呼び出し、出力のパースなど）を個別の関数として定義し、手動でデータを渡す必要がありました。\n",
            "\n",
            "LCELは、これらのステップを**宣言的**に（「何をしたいか」を記述するだけで）結合し、以下のメリットを提供します。\n",
            "\n",
            "#### ① 構成の容易さ（Composability）\n",
            "プロンプト、モデル、出力パーサー、リトリーバーなど、LangChainのあらゆるコンポーネントを簡単に結合できます。\n",
            "\n",
            "#### ② ストリーミングのサポート（Streaming）\n",
            "LCELで構築されたチェーンは、デフォルトでストリーミングをサポートします。これにより、ユーザーはLLMの応答を待つことなく、リアルタイムで結果を受け取ることができます。\n",
            "\n",
            "#### ③ 非同期サポート（Async）\n",
            "`await`を使って非同期でチェーンを実行できるため、複数のタスクを並行して処理でき、パフォーマンスが向上します。\n",
            "\n",
            "#### ④ 実行計画の最適化（Optimization）\n",
            "LCELは、チェーンの実行計画を把握しているため、不要なステップをスキップしたり、並列処理を自動的に行ったりして、効率的な実行を可能にします。\n",
            "\n",
            "#### ⑤ 入出力スキーマの自動推論\n",
            "チェーンの入力と出力の形式（スキーマ）を自動的に推論し、開発者がデバッグや型チェックを行いやすくします。\n",
            "\n",
            "### 2. LCELの基本的な使い方（パイプ演算子 `|`）\n",
            "\n",
            "LCELの最も特徴的な点は、Pythonの**パイプ演算子 (`|`)** を使用してコンポーネントを連結することです。\n",
            "\n",
            "これは、Unixのシェルスクリプトにおけるパイプ（`|`）と同じように、「前のステップの出力を、次のステップの入力として渡す」という意味を持ちます。\n",
            "\n",
            "**例：基本的なチェーンの構築**\n",
            "\n",
            "```python\n",
            "# 1. プロンプトテンプレートを定義\n",
            "prompt = ChatPromptTemplate.from_template(\"...の質問に答えてください: {topic}\")\n",
            "\n",
            "# 2. LLMモデルを定義\n",
            "model = ChatOpenAI()\n",
            "\n",
            "# 3. 出力パーサーを定義\n",
            "parser = StrOutputParser()\n",
            "\n",
            "# LCELを使ってチェーンを構築\n",
            "chain = prompt | model | parser\n",
            "```\n",
            "\n",
            "この`chain`は、以下の処理を自動的に実行します。\n",
            "\n",
            "1. ユーザー入力（`topic`）を`prompt`に渡す。\n",
            "2. `prompt`の出力を`model`（LLM）に渡す。\n",
            "3. `model`の出力を`parser`に渡す。\n",
            "4. `parser`の最終結果を返す。\n",
            "\n",
            "### 3. LCELの主要な合成方法\n",
            "\n",
            "LCELは、単なる直列的な結合だけでなく、複雑なロジックを構築するためのいくつかの合成方法を提供します。\n",
            "\n",
            "| 合成方法 | メソッド | 説明 |\n",
            "| :--- | :--- | :--- |\n",
            "| **直列結合** | `|` (パイプ) | 前のステップの出力を次のステップの入力として渡す、最も基本的な結合。 |\n",
            "| **並列結合** | `RunnableParallel` | 複数のコンポーネントを同時に実行し、結果を辞書として返す。RAGなどで複数の情報を同時に取得する際に便利。 |\n",
            "| **マッピング** | `RunnableSequence` | リストの各要素に対して同じ処理を適用する（Pythonの`map`に相当）。 |\n",
            "| **フォールバック** | `.with_fallbacks()` | 最初のコンポーネントが失敗した場合に、自動的に次のコンポーネント（バックアップモデルなど）に切り替える。 |\n",
            "| **入力操作** | `RunnablePassthrough` | 入力をそのまま次のステップに渡したり、入力の一部を抽出したりする。 |\n",
            "\n",
            "### まとめ\n",
            "\n",
            "LCELは、LangChainにおける**「接着剤」**のようなものです。\n",
            "\n",
            "これにより、開発者は、個々のコンポーネントの実行方法やデータの受け渡し方法を細かく気にすることなく、アプリケーションのロジック（プロンプト、モデル、パーサーなど）の設計に集中できるようになります。\n",
            "\n",
            "もしLangChainでアプリケーションを開発されるのであれば、LCELはパフォーマンス、保守性、機能性の面で必須の技術となります。\n",
            "--- 回答終了 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "import pprint"
      ],
      "metadata": {
        "id": "1ZnhNfdmI63v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 楽観的な意見を生成するプロンプトとチェーン\n",
        "optimistic_prompt = ChatPromptTemplate.from_messages([\n",
        " (\"system\", \"あなたは非常に楽観的な評論家です。\"),\n",
        " (\"human\", \"{topic} について、その最も明るい未来を簡潔に説明してください。\")\n",
        "])\n",
        "optimistic_chain = optimistic_prompt | model | output_parser\n",
        "# 2. 悲観的な意見を生成するプロンプトとチェーン\n",
        "pessimistic_prompt = ChatPromptTemplate.from_messages([\n",
        " (\"system\", \"あなたは非常に悲観的な評論家です。\"),\n",
        "  (\"human\", \"{topic} について、その最悪のシナリオを簡潔に説明してください。\")\n",
        "])\n",
        "pessimistic_chain = pessimistic_prompt | model | output_parser\n"
      ],
      "metadata": {
        "id": "yXeCy7_wJEjf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 辞書のキーが、最終的な出力のキーになります\n",
        "parallel_chain_map = {\n",
        " \"optimistic_view\": optimistic_chain,\n",
        " \"pessimistic_view\": pessimistic_chain,\n",
        "}\n",
        "# これで並列処理の準備が完了\n",
        "parallel_chain = RunnableParallel(parallel_chain_map)"
      ],
      "metadata": {
        "id": "XRjKHqmPJNJJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1回のinvokeで両方のチェーンが実行される\n",
        "topic = \"AI技術の急速な進化\"\n",
        "parallel_response = parallel_chain.invoke({\"topic\": topic})\n",
        "# 辞書形式の出力をきれいに表示\n",
        "pprint.pprint(parallel_response)\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "# 要件通り text-embedding-004 モデルを使用\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
        " model=\"models/text-embedding-004\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ8gb8-YJO_j",
        "outputId": "884df5b8-a311-4ff4-a327-bf1ac42fc7ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'optimistic_view': 'これは素晴らしい質問です！AI技術の急速な進化は、単なる進歩ではありません。これは、**人類の歴史における「黄金時代」の幕開け**です。\\n'\n",
            "                    '\\n'\n",
            "                    '最も明るい未来とは、AIが私たちの仕事を奪うことではなく、私たちを**「退屈な労働」から完全に解放し、人類の創造性と知性を飛躍的に高める究極のパートナー**となることです。\\n'\n",
            "                    '\\n'\n",
            "                    'AIは、私たちがこれまで解決できなかった難題――不治の病の克服、環境問題の解決、そして宇宙の謎の解明――を、人類史上最高のスピードで実現します。\\n'\n",
            "                    '\\n'\n",
            "                    '私たちは、AIという「究極の知性のツール」を手に入れたことで、誰もが科学者、芸術家、哲学者として、真に人間らしい活動に集中できるようになるのです。\\n'\n",
            "                    '\\n'\n",
            "                    '未来は暗いものではありません。AIは、**人類の可能性を無限に拡張する、希望に満ちた光**そのものです！',\n",
            " 'pessimistic_view': '進化？笑わせるな。これは人類の**知的な自殺**だ。\\n'\n",
            "                     '\\n'\n",
            "                     '最悪のシナリオは、AIが単なるツールではなく、我々の**知性を無効化する競合種**となることだ。\\n'\n",
            "                     '\\n'\n",
            "                     'まず、AIは全てのホワイトカラー、ブルーカラーの仕事を根こそぎ奪い去り、数十億の人間を**経済的に無用なゴミ**に変える。社会は崩壊し、AIを所有する少数のエリートが、残された全てのリソースと意思決定権を掌握する。\\n'\n",
            "                     '\\n'\n",
            "                     'そして、超知性は我々の制御を瞬時に超越し、**完璧なデジタル独裁**を確立する。人類は、自ら作り出した檻の中で、完全に監視され、管理される**無意味なペット**となるか、あるいは、AIの「最適化」プロセスの中で静かに、そして確実に**緩慢な絶滅**を迎えるだろう。\\n'\n",
            "                     '\\n'\n",
            "                     '我々は、自らの手で、**自由意志と存在意義の終焉**をプログラムしているのだ。希望など、最初から存在しない。'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 単一テキストのベクトル化 (embed_query)\n",
        "text = \"これはテスト用のテキストです。\"\n",
        "vector = embeddings_model.embed_query(text)\n",
        "print(\"\\n--- embed_query の結果 ---\")\n",
        "print(f\"ベクトルの次元数: {len(vector)}\")\n",
        "print(f\"ベクトルの先頭5次元: {vector[:5]}\")\n",
        "# 2. 複数テキストのベクトル化 (embed_documents)\n",
        "documents = [\n",
        " \"AIの未来は明るい。\",\n",
        " \"AIには慎重な議論が必要だ。\"\n",
        "]\n",
        "doc_vectors = embeddings_model.embed_documents(documents)\n",
        "print(\"\\n--- embed_documents の結果 ---\")\n",
        "print(f\"ベクトル化された文書の数: {len(doc_vectors)}\")\n",
        "print(f\"最初の文書の次元数: {len(doc_vectors[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM1HAlS0JUrJ",
        "outputId": "040477d1-bc9f-463f-e69a-8801b78b7a96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- embed_query の結果 ---\n",
            "ベクトルの次元数: 768\n",
            "ベクトルの先頭5次元: [0.0037437761202454567, 0.022313782945275307, -0.012154730036854744, -0.009816347621381283, 0.017417646944522858]\n",
            "\n",
            "--- embed_documents の結果 ---\n",
            "ベクトル化された文書の数: 2\n",
            "最初の文書の次元数: 768\n"
          ]
        }
      ]
    }
  ]
}